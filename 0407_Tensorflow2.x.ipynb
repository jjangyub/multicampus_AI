{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae8c4032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0          1       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          1       0       0       0       0       0       0       0       0   \n",
       "3          4       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "41995      0       0       0       0       0       0       0       0       0   \n",
       "41996      1       0       0       0       0       0       0       0       0   \n",
       "41997      7       0       0       0       0       0       0       0       0   \n",
       "41998      6       0       0       0       0       0       0       0       0   \n",
       "41999      9       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...         0         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "41995       0  ...         0         0         0         0         0   \n",
       "41996       0  ...         0         0         0         0         0   \n",
       "41997       0  ...         0         0         0         0         0   \n",
       "41998       0  ...         0         0         0         0         0   \n",
       "41999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "41995         0         0         0         0         0  \n",
       "41996         0         0         0         0         0  \n",
       "41997         0         0         0         0         0  \n",
       "41998         0         0         0         0         0  \n",
       "41999         0         0         0         0         0  \n",
       "\n",
       "[42000 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MNIST 예제를 구현해 보아요!\n",
    "# Data는 Kaggle에서 다운로드 할꺼예요!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "display(df)\n",
    "display(df.shape)   # (42000, 785)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e85b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "# 결측치나 이상치 존재X\n",
    "# 단, 정규화는 팔요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a32076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 이미지 확인\n",
    "figure = plt.figure()\n",
    "ax_arr = []  # python list\n",
    "\n",
    "img_data = df.drop('label', axis=1, inplace=False).values\n",
    "\n",
    "for n in range(10):\n",
    "    ax_arr.append(figure.add_subplot(2,5,n+1))\n",
    "    ax_arr[n].imshow(img_data[n].reshape(28,28), \n",
    "                     cmap='Greys',            # 흑백이미지 표현\n",
    "                     interpolation='nearest') # 보간법\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1821d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eec770ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yeop\\AppData\\Local\\Temp\\ipykernel_18072\\2739525339.py:2: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yeop\\AppData\\Local\\Temp\\ipykernel_18072\\2739525339.py:2: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yeop\\AppData\\Local\\Temp\\ipykernel_18072\\2739525339.py:8: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yeop\\AppData\\Local\\Temp\\ipykernel_18072\\2739525339.py:8: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yeop\\AppData\\Local\\Temp\\ipykernel_18072\\2739525339.py:24: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yeop\\AppData\\Local\\Temp\\ipykernel_18072\\2739525339.py:24: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yeop\\AppData\\Local\\Temp\\ipykernel_18072\\2739525339.py:27: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yeop\\AppData\\Local\\Temp\\ipykernel_18072\\2739525339.py:27: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss val : 1.5739980936050415\n",
      "loss val : 0.21712468564510345\n",
      "loss val : 0.20056335628032684\n",
      "loss val : 0.19421109557151794\n",
      "loss val : 0.19195732474327087\n",
      "loss val : 0.1906794160604477\n",
      "loss val : 0.18958503007888794\n",
      "loss val : 0.18855281174182892\n",
      "loss val : 0.18759359419345856\n",
      "loss val : 0.18672561645507812\n"
     ]
    }
   ],
   "source": [
    "## Tensorflow Implementation ##\n",
    "sess = tf.Session()\n",
    "\n",
    "onehot_train_t_data = sess.run(tf.one_hot(train_t_data, depth=10))\n",
    "onehot_test_t_data = sess.run(tf.one_hot(test_t_data, depth=10))\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([784,10]))\n",
    "b = tf.Variable(tf.random.normal([10]))\n",
    "\n",
    "# Hypothesis, Model\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# Loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,\n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# session, 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 반복학습\n",
    "num_of_epoch = 1000\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    \n",
    "    total_batch = int(norm_train_x_data.shape[0] / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_x = norm_train_x_data[i*batch_size:(i+1)*batch_size]\n",
    "        batch_y = onehot_train_t_data[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        _, loss_val = sess.run([train, loss], feed_dict={X:batch_x,\n",
    "                                                         T:batch_y})\n",
    "    if step % 100 == 0:\n",
    "        print('loss val : {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65ad5292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9073809385299683\n"
     ]
    }
   ],
   "source": [
    "# accuracy 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "accuracy_val = sess.run(accuracy, feed_dict={X:norm_test_x_data,\n",
    "                                             T:onehot_test_t_data})\n",
    "print('Accuracy : {}'.format(accuracy_val))  # 0.9073809385299683"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236813f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[1.838462]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)  # 2.3.0\n",
    "\n",
    "W = tf.random.normal([1], dtype=tf.float32)\n",
    "\n",
    "# 1.15버전에서 W의 값을 알아내려면 session을 통해서 node를 실행시켜서\n",
    "# 값을 얻어야 함\n",
    "# 2.x 버전은 eager execution(즉시실행모드)를 지원함\n",
    "# session이 필요없고 일반적인 프로그래밍 하는 것처럼 사용가능\n",
    "\n",
    "print(W.numpy())  # [1.838462]\n",
    "\n",
    "# 추가적으로 초기화 하는 코드 역시 불필요\n",
    "# sess.run(tf.global_varialbes_initializer())  # 사용X\n",
    "\n",
    "# placeholder도 삭제됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2cdce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras이용\n",
    "\n",
    "# keras의 model은 어떻게 만드나요?\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# model.add()를 이용해서 layer 추가\n",
    "# model.add('input layer')\n",
    "# model.add('output layer')\n",
    "\n",
    "# loss 종류와 optimizer종류를 설정\n",
    "# model.compile()\n",
    "\n",
    "# 학습 (마치 sklearn 이용하는 것처럼)\n",
    "# model.fit()\n",
    "\n",
    "# 평가와 predict\n",
    "# model.evaluate()  =>  모델평가\n",
    "# model.predict()   =>  예측값 도출\n",
    "\n",
    "# 모델 저장\n",
    "# model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "658f4c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? \n",
      "Nothing done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0          1       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          1       0       0       0       0       0       0       0       0   \n",
       "3          4       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "41995      0       0       0       0       0       0       0       0       0   \n",
       "41996      1       0       0       0       0       0       0       0       0   \n",
       "41997      7       0       0       0       0       0       0       0       0   \n",
       "41998      6       0       0       0       0       0       0       0       0   \n",
       "41999      9       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...         0         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "41995       0  ...         0         0         0         0         0   \n",
       "41996       0  ...         0         0         0         0         0   \n",
       "41997       0  ...         0         0         0         0         0   \n",
       "41998       0  ...         0         0         0         0         0   \n",
       "41999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "41995         0         0         0         0         0  \n",
       "41996         0         0         0         0         0  \n",
       "41997         0         0         0         0         0  \n",
       "41998         0         0         0         0         0  \n",
       "41999         0         0         0         0         0  \n",
       "\n",
       "[42000 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reset\n",
    "# 대표적인 multinomial 예제인 MNIST를 이용해서\n",
    "# tensorflow 2.x 버전으로 구현해보자\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential  # kears model  # 레이어를 층층히 쌓아가는 연쇄 모델\n",
    "from tensorflow.keras.layers import Flatten, Dense  # Flatten(Input Layer)\n",
    "                                                     # Dense(Output Layer)  # 완전연결층\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# raw data loading\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3198ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "# Data Split\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9977ad9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# tensorflow 2.x구현\n",
    "\n",
    "# model 생성\n",
    "model = Sequential()\n",
    "\n",
    "# layer 추가\n",
    "# input layer\n",
    "model.add(Flatten(input_shape=(norm_train_x_data.shape[1],)))\n",
    "# input shape은 독립변수 갯수\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=10,\n",
    "                activation='softmax'))  # units의 값은 class 값의 갯수\n",
    "\n",
    "# input layer는 사실 하는 일이 없음! 그래서 코드를 나눠쓰지 않고 한번에 쓸 수 있음 (나중에)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6522de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 2.2571 - accuracy: 0.2072 - val_loss: 2.1224 - val_accuracy: 0.3168\n",
      "Epoch 2/100\n",
      "236/236 [==============================] - 0s 864us/step - loss: 2.0299 - accuracy: 0.3848 - val_loss: 1.9209 - val_accuracy: 0.4679\n",
      "Epoch 3/100\n",
      "236/236 [==============================] - 0s 853us/step - loss: 1.8463 - accuracy: 0.5214 - val_loss: 1.7545 - val_accuracy: 0.5811\n",
      "Epoch 4/100\n",
      "236/236 [==============================] - 0s 818us/step - loss: 1.6935 - accuracy: 0.6080 - val_loss: 1.6153 - val_accuracy: 0.6534\n",
      "Epoch 5/100\n",
      "236/236 [==============================] - 0s 786us/step - loss: 1.5653 - accuracy: 0.6613 - val_loss: 1.4982 - val_accuracy: 0.6940\n",
      "Epoch 6/100\n",
      "236/236 [==============================] - 0s 842us/step - loss: 1.4571 - accuracy: 0.6940 - val_loss: 1.3990 - val_accuracy: 0.7207\n",
      "Epoch 7/100\n",
      "236/236 [==============================] - 0s 824us/step - loss: 1.3653 - accuracy: 0.7170 - val_loss: 1.3147 - val_accuracy: 0.7388\n",
      "Epoch 8/100\n",
      "236/236 [==============================] - 0s 822us/step - loss: 1.2869 - accuracy: 0.7346 - val_loss: 1.2424 - val_accuracy: 0.7558\n",
      "Epoch 9/100\n",
      "236/236 [==============================] - 0s 780us/step - loss: 1.2194 - accuracy: 0.7506 - val_loss: 1.1798 - val_accuracy: 0.7655\n",
      "Epoch 10/100\n",
      "236/236 [==============================] - 0s 796us/step - loss: 1.1609 - accuracy: 0.7635 - val_loss: 1.1255 - val_accuracy: 0.7769\n",
      "Epoch 11/100\n",
      "236/236 [==============================] - 0s 781us/step - loss: 1.1098 - accuracy: 0.7747 - val_loss: 1.0779 - val_accuracy: 0.7859\n",
      "Epoch 12/100\n",
      "236/236 [==============================] - 0s 780us/step - loss: 1.0649 - accuracy: 0.7822 - val_loss: 1.0359 - val_accuracy: 0.7956\n",
      "Epoch 13/100\n",
      "236/236 [==============================] - 0s 749us/step - loss: 1.0250 - accuracy: 0.7895 - val_loss: 0.9985 - val_accuracy: 0.8024\n",
      "Epoch 14/100\n",
      "236/236 [==============================] - 0s 770us/step - loss: 0.9895 - accuracy: 0.7965 - val_loss: 0.9652 - val_accuracy: 0.8088\n",
      "Epoch 15/100\n",
      "236/236 [==============================] - 0s 799us/step - loss: 0.9576 - accuracy: 0.8026 - val_loss: 0.9351 - val_accuracy: 0.8131\n",
      "Epoch 16/100\n",
      "236/236 [==============================] - 0s 776us/step - loss: 0.9289 - accuracy: 0.8072 - val_loss: 0.9080 - val_accuracy: 0.8170\n",
      "Epoch 17/100\n",
      "236/236 [==============================] - 0s 747us/step - loss: 0.9029 - accuracy: 0.8108 - val_loss: 0.8834 - val_accuracy: 0.8211\n",
      "Epoch 18/100\n",
      "236/236 [==============================] - 0s 810us/step - loss: 0.8793 - accuracy: 0.8150 - val_loss: 0.8609 - val_accuracy: 0.8259\n",
      "Epoch 19/100\n",
      "236/236 [==============================] - 0s 799us/step - loss: 0.8576 - accuracy: 0.8186 - val_loss: 0.8404 - val_accuracy: 0.8296\n",
      "Epoch 20/100\n",
      "236/236 [==============================] - 0s 813us/step - loss: 0.8377 - accuracy: 0.8222 - val_loss: 0.8215 - val_accuracy: 0.8311\n",
      "Epoch 21/100\n",
      "236/236 [==============================] - 0s 779us/step - loss: 0.8194 - accuracy: 0.8248 - val_loss: 0.8040 - val_accuracy: 0.8327\n",
      "Epoch 22/100\n",
      "236/236 [==============================] - 0s 834us/step - loss: 0.8024 - accuracy: 0.8274 - val_loss: 0.7880 - val_accuracy: 0.8340\n",
      "Epoch 23/100\n",
      "236/236 [==============================] - 0s 856us/step - loss: 0.7867 - accuracy: 0.8299 - val_loss: 0.7730 - val_accuracy: 0.8371\n",
      "Epoch 24/100\n",
      "236/236 [==============================] - 0s 949us/step - loss: 0.7720 - accuracy: 0.8326 - val_loss: 0.7590 - val_accuracy: 0.8386\n",
      "Epoch 25/100\n",
      "236/236 [==============================] - 0s 913us/step - loss: 0.7584 - accuracy: 0.8341 - val_loss: 0.7460 - val_accuracy: 0.8405\n",
      "Epoch 26/100\n",
      "236/236 [==============================] - 0s 878us/step - loss: 0.7456 - accuracy: 0.8361 - val_loss: 0.7338 - val_accuracy: 0.8425\n",
      "Epoch 27/100\n",
      "236/236 [==============================] - 0s 849us/step - loss: 0.7336 - accuracy: 0.8384 - val_loss: 0.7223 - val_accuracy: 0.8446\n",
      "Epoch 28/100\n",
      "236/236 [==============================] - 0s 833us/step - loss: 0.7224 - accuracy: 0.8407 - val_loss: 0.7115 - val_accuracy: 0.8457\n",
      "Epoch 29/100\n",
      "236/236 [==============================] - 0s 855us/step - loss: 0.7118 - accuracy: 0.8423 - val_loss: 0.7014 - val_accuracy: 0.8474\n",
      "Epoch 30/100\n",
      "236/236 [==============================] - 0s 836us/step - loss: 0.7018 - accuracy: 0.8435 - val_loss: 0.6919 - val_accuracy: 0.8485\n",
      "Epoch 31/100\n",
      "236/236 [==============================] - 0s 814us/step - loss: 0.6923 - accuracy: 0.8455 - val_loss: 0.6829 - val_accuracy: 0.8493\n",
      "Epoch 32/100\n",
      "236/236 [==============================] - 0s 761us/step - loss: 0.6833 - accuracy: 0.8467 - val_loss: 0.6742 - val_accuracy: 0.8498\n",
      "Epoch 33/100\n",
      "236/236 [==============================] - 0s 839us/step - loss: 0.6748 - accuracy: 0.8482 - val_loss: 0.6662 - val_accuracy: 0.8509\n",
      "Epoch 34/100\n",
      "236/236 [==============================] - 0s 817us/step - loss: 0.6668 - accuracy: 0.8491 - val_loss: 0.6584 - val_accuracy: 0.8520\n",
      "Epoch 35/100\n",
      "236/236 [==============================] - 0s 850us/step - loss: 0.6591 - accuracy: 0.8509 - val_loss: 0.6511 - val_accuracy: 0.8536\n",
      "Epoch 36/100\n",
      "236/236 [==============================] - 0s 863us/step - loss: 0.6518 - accuracy: 0.8521 - val_loss: 0.6440 - val_accuracy: 0.8544\n",
      "Epoch 37/100\n",
      "236/236 [==============================] - 0s 853us/step - loss: 0.6448 - accuracy: 0.8537 - val_loss: 0.6374 - val_accuracy: 0.8553\n",
      "Epoch 38/100\n",
      "236/236 [==============================] - 0s 919us/step - loss: 0.6381 - accuracy: 0.8547 - val_loss: 0.6310 - val_accuracy: 0.8561\n",
      "Epoch 39/100\n",
      "236/236 [==============================] - 0s 845us/step - loss: 0.6317 - accuracy: 0.8552 - val_loss: 0.6249 - val_accuracy: 0.8587\n",
      "Epoch 40/100\n",
      "236/236 [==============================] - 0s 861us/step - loss: 0.6256 - accuracy: 0.8563 - val_loss: 0.6190 - val_accuracy: 0.8590\n",
      "Epoch 41/100\n",
      "236/236 [==============================] - 0s 851us/step - loss: 0.6197 - accuracy: 0.8575 - val_loss: 0.6134 - val_accuracy: 0.8602\n",
      "Epoch 42/100\n",
      "236/236 [==============================] - 0s 858us/step - loss: 0.6141 - accuracy: 0.8580 - val_loss: 0.6081 - val_accuracy: 0.8609\n",
      "Epoch 43/100\n",
      "236/236 [==============================] - 0s 871us/step - loss: 0.6087 - accuracy: 0.8587 - val_loss: 0.6029 - val_accuracy: 0.8611\n",
      "Epoch 44/100\n",
      "236/236 [==============================] - 0s 863us/step - loss: 0.6035 - accuracy: 0.8594 - val_loss: 0.5980 - val_accuracy: 0.8619\n",
      "Epoch 45/100\n",
      "236/236 [==============================] - 0s 861us/step - loss: 0.5985 - accuracy: 0.8597 - val_loss: 0.5932 - val_accuracy: 0.8626\n",
      "Epoch 46/100\n",
      "236/236 [==============================] - 0s 839us/step - loss: 0.5937 - accuracy: 0.8606 - val_loss: 0.5886 - val_accuracy: 0.8631\n",
      "Epoch 47/100\n",
      "236/236 [==============================] - 0s 975us/step - loss: 0.5891 - accuracy: 0.8612 - val_loss: 0.5841 - val_accuracy: 0.8639\n",
      "Epoch 48/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5846 - accuracy: 0.8622 - val_loss: 0.5799 - val_accuracy: 0.8646\n",
      "Epoch 49/100\n",
      "236/236 [==============================] - 0s 924us/step - loss: 0.5803 - accuracy: 0.8626 - val_loss: 0.5757 - val_accuracy: 0.8650\n",
      "Epoch 50/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 0.5741 - accuracy: 0.86 - 0s 1ms/step - loss: 0.5761 - accuracy: 0.8636 - val_loss: 0.5717 - val_accuracy: 0.8648\n",
      "Epoch 51/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5720 - accuracy: 0.8639 - val_loss: 0.5679 - val_accuracy: 0.8651\n",
      "Epoch 52/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5681 - accuracy: 0.8645 - val_loss: 0.5642 - val_accuracy: 0.8653\n",
      "Epoch 53/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5644 - accuracy: 0.8656 - val_loss: 0.5606 - val_accuracy: 0.8658\n",
      "Epoch 54/100\n",
      "236/236 [==============================] - 0s 973us/step - loss: 0.5607 - accuracy: 0.8660 - val_loss: 0.5571 - val_accuracy: 0.8660\n",
      "Epoch 55/100\n",
      "236/236 [==============================] - 0s 785us/step - loss: 0.5572 - accuracy: 0.8665 - val_loss: 0.5537 - val_accuracy: 0.8665\n",
      "Epoch 56/100\n",
      "236/236 [==============================] - 0s 769us/step - loss: 0.5537 - accuracy: 0.8667 - val_loss: 0.5504 - val_accuracy: 0.8670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "236/236 [==============================] - 0s 776us/step - loss: 0.5504 - accuracy: 0.8672 - val_loss: 0.5472 - val_accuracy: 0.8673\n",
      "Epoch 58/100\n",
      "236/236 [==============================] - 0s 785us/step - loss: 0.5471 - accuracy: 0.8676 - val_loss: 0.5441 - val_accuracy: 0.8677\n",
      "Epoch 59/100\n",
      "236/236 [==============================] - 0s 786us/step - loss: 0.5440 - accuracy: 0.8679 - val_loss: 0.5411 - val_accuracy: 0.8679\n",
      "Epoch 60/100\n",
      "236/236 [==============================] - 0s 773us/step - loss: 0.5409 - accuracy: 0.8685 - val_loss: 0.5382 - val_accuracy: 0.8682\n",
      "Epoch 61/100\n",
      "236/236 [==============================] - 0s 821us/step - loss: 0.5379 - accuracy: 0.8689 - val_loss: 0.5353 - val_accuracy: 0.8685\n",
      "Epoch 62/100\n",
      "236/236 [==============================] - 0s 753us/step - loss: 0.5350 - accuracy: 0.8698 - val_loss: 0.5326 - val_accuracy: 0.8694\n",
      "Epoch 63/100\n",
      "236/236 [==============================] - 0s 753us/step - loss: 0.5322 - accuracy: 0.8702 - val_loss: 0.5299 - val_accuracy: 0.8701\n",
      "Epoch 64/100\n",
      "236/236 [==============================] - 0s 751us/step - loss: 0.5295 - accuracy: 0.8705 - val_loss: 0.5273 - val_accuracy: 0.8706\n",
      "Epoch 65/100\n",
      "236/236 [==============================] - 0s 750us/step - loss: 0.5268 - accuracy: 0.8711 - val_loss: 0.5247 - val_accuracy: 0.8711\n",
      "Epoch 66/100\n",
      "236/236 [==============================] - 0s 769us/step - loss: 0.5242 - accuracy: 0.8717 - val_loss: 0.5223 - val_accuracy: 0.8714\n",
      "Epoch 67/100\n",
      "236/236 [==============================] - 0s 737us/step - loss: 0.5216 - accuracy: 0.8723 - val_loss: 0.5199 - val_accuracy: 0.8714\n",
      "Epoch 68/100\n",
      "236/236 [==============================] - 0s 743us/step - loss: 0.5192 - accuracy: 0.8722 - val_loss: 0.5175 - val_accuracy: 0.8719\n",
      "Epoch 69/100\n",
      "236/236 [==============================] - 0s 740us/step - loss: 0.5167 - accuracy: 0.8728 - val_loss: 0.5152 - val_accuracy: 0.8719\n",
      "Epoch 70/100\n",
      "236/236 [==============================] - 0s 840us/step - loss: 0.5144 - accuracy: 0.8733 - val_loss: 0.5129 - val_accuracy: 0.8726\n",
      "Epoch 71/100\n",
      "236/236 [==============================] - 0s 896us/step - loss: 0.5121 - accuracy: 0.8734 - val_loss: 0.5107 - val_accuracy: 0.8728\n",
      "Epoch 72/100\n",
      "236/236 [==============================] - 0s 908us/step - loss: 0.5098 - accuracy: 0.8739 - val_loss: 0.5086 - val_accuracy: 0.8730\n",
      "Epoch 73/100\n",
      "236/236 [==============================] - 0s 885us/step - loss: 0.5076 - accuracy: 0.8746 - val_loss: 0.5065 - val_accuracy: 0.8733\n",
      "Epoch 74/100\n",
      "236/236 [==============================] - 0s 928us/step - loss: 0.5054 - accuracy: 0.8746 - val_loss: 0.5045 - val_accuracy: 0.8738\n",
      "Epoch 75/100\n",
      "236/236 [==============================] - 0s 842us/step - loss: 0.5033 - accuracy: 0.8751 - val_loss: 0.5025 - val_accuracy: 0.8736\n",
      "Epoch 76/100\n",
      "236/236 [==============================] - 0s 769us/step - loss: 0.5013 - accuracy: 0.8759 - val_loss: 0.5005 - val_accuracy: 0.8741\n",
      "Epoch 77/100\n",
      "236/236 [==============================] - 0s 815us/step - loss: 0.4993 - accuracy: 0.8759 - val_loss: 0.4986 - val_accuracy: 0.8745\n",
      "Epoch 78/100\n",
      "236/236 [==============================] - 0s 806us/step - loss: 0.4973 - accuracy: 0.8760 - val_loss: 0.4967 - val_accuracy: 0.8752\n",
      "Epoch 79/100\n",
      "236/236 [==============================] - 0s 788us/step - loss: 0.4954 - accuracy: 0.8764 - val_loss: 0.4949 - val_accuracy: 0.8753\n",
      "Epoch 80/100\n",
      "236/236 [==============================] - 0s 855us/step - loss: 0.4935 - accuracy: 0.8768 - val_loss: 0.4931 - val_accuracy: 0.8759\n",
      "Epoch 81/100\n",
      "236/236 [==============================] - 0s 958us/step - loss: 0.4916 - accuracy: 0.8770 - val_loss: 0.4913 - val_accuracy: 0.8764\n",
      "Epoch 82/100\n",
      "236/236 [==============================] - 0s 903us/step - loss: 0.4898 - accuracy: 0.8774 - val_loss: 0.4896 - val_accuracy: 0.8764\n",
      "Epoch 83/100\n",
      "236/236 [==============================] - 0s 884us/step - loss: 0.4880 - accuracy: 0.8775 - val_loss: 0.4879 - val_accuracy: 0.8767\n",
      "Epoch 84/100\n",
      "236/236 [==============================] - 0s 908us/step - loss: 0.4863 - accuracy: 0.8776 - val_loss: 0.4863 - val_accuracy: 0.8776\n",
      "Epoch 85/100\n",
      "236/236 [==============================] - 0s 741us/step - loss: 0.4846 - accuracy: 0.8781 - val_loss: 0.4846 - val_accuracy: 0.8772\n",
      "Epoch 86/100\n",
      "236/236 [==============================] - 0s 749us/step - loss: 0.4829 - accuracy: 0.8782 - val_loss: 0.4831 - val_accuracy: 0.8777\n",
      "Epoch 87/100\n",
      "236/236 [==============================] - 0s 746us/step - loss: 0.4813 - accuracy: 0.8785 - val_loss: 0.4815 - val_accuracy: 0.8782\n",
      "Epoch 88/100\n",
      "236/236 [==============================] - 0s 859us/step - loss: 0.4796 - accuracy: 0.8787 - val_loss: 0.4800 - val_accuracy: 0.8784\n",
      "Epoch 89/100\n",
      "236/236 [==============================] - 0s 874us/step - loss: 0.4781 - accuracy: 0.8787 - val_loss: 0.4785 - val_accuracy: 0.8786\n",
      "Epoch 90/100\n",
      "236/236 [==============================] - 0s 793us/step - loss: 0.4765 - accuracy: 0.8788 - val_loss: 0.4770 - val_accuracy: 0.8787\n",
      "Epoch 91/100\n",
      "236/236 [==============================] - 0s 773us/step - loss: 0.4750 - accuracy: 0.8792 - val_loss: 0.4756 - val_accuracy: 0.8793\n",
      "Epoch 92/100\n",
      "236/236 [==============================] - 0s 795us/step - loss: 0.4735 - accuracy: 0.8793 - val_loss: 0.4741 - val_accuracy: 0.8789\n",
      "Epoch 93/100\n",
      "236/236 [==============================] - 0s 830us/step - loss: 0.4720 - accuracy: 0.8794 - val_loss: 0.4728 - val_accuracy: 0.8793\n",
      "Epoch 94/100\n",
      "236/236 [==============================] - 0s 840us/step - loss: 0.4706 - accuracy: 0.8797 - val_loss: 0.4714 - val_accuracy: 0.8793\n",
      "Epoch 95/100\n",
      "236/236 [==============================] - 0s 812us/step - loss: 0.4691 - accuracy: 0.8797 - val_loss: 0.4701 - val_accuracy: 0.8796\n",
      "Epoch 96/100\n",
      "236/236 [==============================] - 0s 771us/step - loss: 0.4677 - accuracy: 0.8798 - val_loss: 0.4688 - val_accuracy: 0.8799\n",
      "Epoch 97/100\n",
      "236/236 [==============================] - 0s 783us/step - loss: 0.4664 - accuracy: 0.8802 - val_loss: 0.4675 - val_accuracy: 0.8803\n",
      "Epoch 98/100\n",
      "236/236 [==============================] - 0s 819us/step - loss: 0.4650 - accuracy: 0.8802 - val_loss: 0.4662 - val_accuracy: 0.8803\n",
      "Epoch 99/100\n",
      "236/236 [==============================] - 0s 834us/step - loss: 0.4637 - accuracy: 0.8807 - val_loss: 0.4649 - val_accuracy: 0.8801\n",
      "Epoch 100/100\n",
      "236/236 [==============================] - 0s 771us/step - loss: 0.4624 - accuracy: 0.8808 - val_loss: 0.4637 - val_accuracy: 0.8804\n"
     ]
    }
   ],
   "source": [
    "# model compile\n",
    "# 사용할 loss함수를 지정, 사용한 optimizer(알고리즘)를 지정\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# loss\n",
    "# linear : linear regression의 loss (MSE)\n",
    "# binary classfication : binary_crossentropy\n",
    "# multinomial classification : categorical_crossentropy (onehot encoding처리를 해야함)\n",
    "# multinomial classification : sparse_categorical_crossentropy (one-hot 처리 필요X)\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 학습결과를 변수에 저장\n",
    "history = model.fit(norm_train_x_data,\n",
    "                    train_t_data,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dda94c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/394 [..............................] - ETA: 0s - loss: 0.5764 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0011s). Check your callbacks.\n",
      "394/394 [==============================] - 0s 373us/step - loss: 0.4780 - accuracy: 0.8764\n",
      "[0.4780237078666687, 0.8764285445213318]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47583842277526855, 0.8765873312950134]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.evaluate(norm_test_x_data, test_t_data))\n",
    "#    loss                  accuracy\n",
    "[0.47583842277526855, 0.8765873312950134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6853f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 만든 모델을 저장해보자\n",
    "# 학습 한 수 모델이 메모리에 저장되어 있음. 프로그램 종료하면 다 날라감!\n",
    "# 내일 다시 하려면 처음부터 다시 학습해야함! => 시간이 오래걸림\n",
    "\n",
    "# 모델학습에 시간이 너무 오래걸리는 경우\n",
    "# 중간에 미리 저장해 놓으면 거기서부터 재 학습 가능\n",
    "\n",
    "# 다른 사람과 모델 공유 가능\n",
    "\n",
    "# 저장할 때 2가지 방법이 있음\n",
    "# 모델을 저장할 때 모델 구조와 계산된 W,b를 같이 저장가능\n",
    "# 장점: 편함 , 단점: 사이즈가 큼\n",
    "\n",
    "# 모델을 저장할 때 모델 구조는 저장하지 않고 W,b만 저장\n",
    "# 장점: 크기가 작음, 단점: 사용하려면 모델을 먼저 만들고 W,b를 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fd0f25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 2.3394 - accuracy: 0.1449\n",
      "Epoch 00001: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 2.3394 - accuracy: 0.1449 - val_loss: 2.2087 - val_accuracy: 0.2248\n",
      "Epoch 2/100\n",
      "195/236 [=======================>......] - ETA: 0s - loss: 2.1160 - accuracy: 0.2857\n",
      "Epoch 00002: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 2.0958 - accuracy: 0.3018 - val_loss: 1.9944 - val_accuracy: 0.3769\n",
      "Epoch 3/100\n",
      "218/236 [==========================>...] - ETA: 0s - loss: 1.9070 - accuracy: 0.4544\n",
      "Epoch 00003: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 924us/step - loss: 1.9008 - accuracy: 0.4594 - val_loss: 1.8165 - val_accuracy: 0.5262\n",
      "Epoch 4/100\n",
      "207/236 [=========================>....] - ETA: 0s - loss: 1.7466 - accuracy: 0.5826\n",
      "Epoch 00004: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 982us/step - loss: 1.7380 - accuracy: 0.5866 - val_loss: 1.6673 - val_accuracy: 0.6194\n",
      "Epoch 5/100\n",
      "195/236 [=======================>......] - ETA: 0s - loss: 1.6117 - accuracy: 0.6521\n",
      "Epoch 00005: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.6014 - accuracy: 0.6555 - val_loss: 1.5418 - val_accuracy: 0.6753\n",
      "Epoch 6/100\n",
      "183/236 [======================>.......] - ETA: 0s - loss: 1.4982 - accuracy: 0.6920\n",
      "Epoch 00006: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.4864 - accuracy: 0.6961 - val_loss: 1.4359 - val_accuracy: 0.7150\n",
      "Epoch 7/100\n",
      "208/236 [=========================>....] - ETA: 0s - loss: 1.3950 - accuracy: 0.7248\n",
      "Epoch 00007: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.3891 - accuracy: 0.7269 - val_loss: 1.3461 - val_accuracy: 0.7379\n",
      "Epoch 8/100\n",
      "191/236 [=======================>......] - ETA: 0s - loss: 1.3145 - accuracy: 0.7454\n",
      "Epoch 00008: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.3063 - accuracy: 0.7471 - val_loss: 1.2694 - val_accuracy: 0.7536\n",
      "Epoch 9/100\n",
      "201/236 [========================>.....] - ETA: 0s - loss: 1.2408 - accuracy: 0.7600\n",
      "Epoch 00009: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.2354 - accuracy: 0.7614 - val_loss: 1.2035 - val_accuracy: 0.7665\n",
      "Epoch 10/100\n",
      "201/236 [========================>.....] - ETA: 0s - loss: 1.1778 - accuracy: 0.7712\n",
      "Epoch 00010: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.1741 - accuracy: 0.7729 - val_loss: 1.1463 - val_accuracy: 0.7786\n",
      "Epoch 11/100\n",
      "199/236 [========================>.....] - ETA: 0s - loss: 1.1222 - accuracy: 0.7839\n",
      "Epoch 00011: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.1208 - accuracy: 0.7833 - val_loss: 1.0963 - val_accuracy: 0.7889\n",
      "Epoch 12/100\n",
      "210/236 [=========================>....] - ETA: 0s - loss: 1.0778 - accuracy: 0.7910\n",
      "Epoch 00012: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 953us/step - loss: 1.0740 - accuracy: 0.7928 - val_loss: 1.0525 - val_accuracy: 0.7963\n",
      "Epoch 13/100\n",
      "188/236 [======================>.......] - ETA: 0s - loss: 1.0353 - accuracy: 0.7973\n",
      "Epoch 00013: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.0328 - accuracy: 0.7985 - val_loss: 1.0135 - val_accuracy: 0.8027\n",
      "Epoch 14/100\n",
      "194/236 [=======================>......] - ETA: 0s - loss: 0.9982 - accuracy: 0.8059\n",
      "Epoch 00014: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9961 - accuracy: 0.8046 - val_loss: 0.9789 - val_accuracy: 0.8075\n",
      "Epoch 15/100\n",
      "211/236 [=========================>....] - ETA: 0s - loss: 0.9666 - accuracy: 0.8092\n",
      "Epoch 00015: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9634 - accuracy: 0.8103 - val_loss: 0.9478 - val_accuracy: 0.8133\n",
      "Epoch 16/100\n",
      "194/236 [=======================>......] - ETA: 0s - loss: 0.9371 - accuracy: 0.8156\n",
      "Epoch 00016: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9339 - accuracy: 0.8153 - val_loss: 0.9199 - val_accuracy: 0.8175\n",
      "Epoch 17/100\n",
      "210/236 [=========================>....] - ETA: 0s - loss: 0.9081 - accuracy: 0.8202\n",
      "Epoch 00017: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9073 - accuracy: 0.8198 - val_loss: 0.8944 - val_accuracy: 0.8199\n",
      "Epoch 18/100\n",
      "186/236 [======================>.......] - ETA: 0s - loss: 0.8844 - accuracy: 0.8215\n",
      "Epoch 00018: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8831 - accuracy: 0.8225 - val_loss: 0.8714 - val_accuracy: 0.8245\n",
      "Epoch 19/100\n",
      "208/236 [=========================>....] - ETA: 0s - loss: 0.8620 - accuracy: 0.8255\n",
      "Epoch 00019: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8610 - accuracy: 0.8254 - val_loss: 0.8503 - val_accuracy: 0.8274\n",
      "Epoch 20/100\n",
      "186/236 [======================>.......] - ETA: 0s - loss: 0.8427 - accuracy: 0.8262\n",
      "Epoch 00020: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8408 - accuracy: 0.8272 - val_loss: 0.8310 - val_accuracy: 0.8304\n",
      "Epoch 21/100\n",
      "223/236 [===========================>..] - ETA: 0s - loss: 0.8244 - accuracy: 0.8286\n",
      "Epoch 00021: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 922us/step - loss: 0.8222 - accuracy: 0.8299 - val_loss: 0.8132 - val_accuracy: 0.8344\n",
      "Epoch 22/100\n",
      "199/236 [========================>.....] - ETA: 0s - loss: 0.8074 - accuracy: 0.8308\n",
      "Epoch 00022: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8050 - accuracy: 0.8315 - val_loss: 0.7968 - val_accuracy: 0.8369\n",
      "Epoch 23/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.7892 - accuracy: 0.8339\n",
      "Epoch 00023: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7891 - accuracy: 0.8341 - val_loss: 0.7816 - val_accuracy: 0.8384\n",
      "Epoch 24/100\n",
      "178/236 [=====================>........] - ETA: 0s - loss: 0.7789 - accuracy: 0.8335\n",
      "Epoch 00024: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7743 - accuracy: 0.8360 - val_loss: 0.7674 - val_accuracy: 0.8406\n",
      "Epoch 25/100\n",
      "188/236 [======================>.......] - ETA: 0s - loss: 0.7585 - accuracy: 0.8414\n",
      "Epoch 00025: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7606 - accuracy: 0.8382 - val_loss: 0.7541 - val_accuracy: 0.8422\n",
      "Epoch 26/100\n",
      "175/236 [=====================>........] - ETA: 0s - loss: 0.7506 - accuracy: 0.8398\n",
      "Epoch 00026: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7477 - accuracy: 0.8397 - val_loss: 0.7417 - val_accuracy: 0.8425\n",
      "Epoch 27/100\n",
      "191/236 [=======================>......] - ETA: 0s - loss: 0.7384 - accuracy: 0.8395\n",
      "Epoch 00027: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7356 - accuracy: 0.8417 - val_loss: 0.7301 - val_accuracy: 0.8432\n",
      "Epoch 28/100\n",
      "209/236 [=========================>....] - ETA: 0s - loss: 0.7254 - accuracy: 0.8431\n",
      "Epoch 00028: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7243 - accuracy: 0.8433 - val_loss: 0.7192 - val_accuracy: 0.8440\n",
      "Epoch 29/100\n",
      "185/236 [======================>.......] - ETA: 0s - loss: 0.7145 - accuracy: 0.8447\n",
      "Epoch 00029: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7136 - accuracy: 0.8447 - val_loss: 0.7090 - val_accuracy: 0.8454\n",
      "Epoch 30/100\n",
      "186/236 [======================>.......] - ETA: 0s - loss: 0.7034 - accuracy: 0.8468\n",
      "Epoch 00030: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7036 - accuracy: 0.8460 - val_loss: 0.6993 - val_accuracy: 0.8457\n",
      "Epoch 31/100\n",
      "198/236 [========================>.....] - ETA: 0s - loss: 0.6949 - accuracy: 0.8471\n",
      "Epoch 00031: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6941 - accuracy: 0.8476 - val_loss: 0.6902 - val_accuracy: 0.8476\n",
      "Epoch 32/100\n",
      "211/236 [=========================>....] - ETA: 0s - loss: 0.6850 - accuracy: 0.8480\n",
      "Epoch 00032: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6851 - accuracy: 0.8484 - val_loss: 0.6815 - val_accuracy: 0.8490\n",
      "Epoch 33/100\n",
      "214/236 [==========================>...] - ETA: 0s - loss: 0.6764 - accuracy: 0.8496\n",
      "Epoch 00033: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6766 - accuracy: 0.8500 - val_loss: 0.6733 - val_accuracy: 0.8503\n",
      "Epoch 34/100\n",
      "185/236 [======================>.......] - ETA: 0s - loss: 0.6675 - accuracy: 0.8503\n",
      "Epoch 00034: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6685 - accuracy: 0.8502 - val_loss: 0.6655 - val_accuracy: 0.8509\n",
      "Epoch 35/100\n",
      "232/236 [============================>.] - ETA: 0s - loss: 0.6605 - accuracy: 0.8512\n",
      "Epoch 00035: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6608 - accuracy: 0.8512 - val_loss: 0.6580 - val_accuracy: 0.8522\n",
      "Epoch 36/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.6538 - accuracy: 0.8522\n",
      "Epoch 00036: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6535 - accuracy: 0.8523 - val_loss: 0.6510 - val_accuracy: 0.8532\n",
      "Epoch 37/100\n",
      "196/236 [=======================>......] - ETA: 0s - loss: 0.6490 - accuracy: 0.8519\n",
      "Epoch 00037: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6465 - accuracy: 0.8532 - val_loss: 0.6442 - val_accuracy: 0.8541\n",
      "Epoch 38/100\n",
      "191/236 [=======================>......] - ETA: 0s - loss: 0.6413 - accuracy: 0.8547\n",
      "Epoch 00038: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6398 - accuracy: 0.8546 - val_loss: 0.6377 - val_accuracy: 0.8551\n",
      "Epoch 39/100\n",
      "210/236 [=========================>....] - ETA: 0s - loss: 0.6338 - accuracy: 0.8542\n",
      "Epoch 00039: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6334 - accuracy: 0.8552 - val_loss: 0.6316 - val_accuracy: 0.8565\n",
      "Epoch 40/100\n",
      "183/236 [======================>.......] - ETA: 0s - loss: 0.6280 - accuracy: 0.8556\n",
      "Epoch 00040: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6273 - accuracy: 0.8560 - val_loss: 0.6257 - val_accuracy: 0.8566\n",
      "Epoch 41/100\n",
      "180/236 [=====================>........] - ETA: 0s - loss: 0.6238 - accuracy: 0.8551\n",
      "Epoch 00041: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6214 - accuracy: 0.8567 - val_loss: 0.6200 - val_accuracy: 0.8570\n",
      "Epoch 42/100\n",
      "180/236 [=====================>........] - ETA: 0s - loss: 0.6156 - accuracy: 0.8585\n",
      "Epoch 00042: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6158 - accuracy: 0.8578 - val_loss: 0.6146 - val_accuracy: 0.8578\n",
      "Epoch 43/100\n",
      "183/236 [======================>.......] - ETA: 0s - loss: 0.6089 - accuracy: 0.8589\n",
      "Epoch 00043: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6104 - accuracy: 0.8582 - val_loss: 0.6094 - val_accuracy: 0.8587\n",
      "Epoch 44/100\n",
      "198/236 [========================>.....] - ETA: 0s - loss: 0.6068 - accuracy: 0.8595\n",
      "Epoch 00044: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6052 - accuracy: 0.8588 - val_loss: 0.6044 - val_accuracy: 0.8587\n",
      "Epoch 45/100\n",
      "188/236 [======================>.......] - ETA: 0s - loss: 0.6048 - accuracy: 0.8587\n",
      "Epoch 00045: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6002 - accuracy: 0.8599 - val_loss: 0.5996 - val_accuracy: 0.8594\n",
      "Epoch 46/100\n",
      "229/236 [============================>.] - ETA: 0s - loss: 0.5954 - accuracy: 0.8615\n",
      "Epoch 00046: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5954 - accuracy: 0.8612 - val_loss: 0.5949 - val_accuracy: 0.8594\n",
      "Epoch 47/100\n",
      "181/236 [======================>.......] - ETA: 0s - loss: 0.5915 - accuracy: 0.8615\n",
      "Epoch 00047: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5908 - accuracy: 0.8619 - val_loss: 0.5905 - val_accuracy: 0.8597\n",
      "Epoch 48/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.5863 - accuracy: 0.8624\n",
      "Epoch 00048: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5863 - accuracy: 0.8625 - val_loss: 0.5861 - val_accuracy: 0.8609\n",
      "Epoch 49/100\n",
      "207/236 [=========================>....] - ETA: 0s - loss: 0.5770 - accuracy: 0.8658\n",
      "Epoch 00049: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5820 - accuracy: 0.8631 - val_loss: 0.5820 - val_accuracy: 0.8619\n",
      "Epoch 50/100\n",
      "204/236 [========================>.....] - ETA: 0s - loss: 0.5781 - accuracy: 0.8635\n",
      "Epoch 00050: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5778 - accuracy: 0.8641 - val_loss: 0.5779 - val_accuracy: 0.8624\n",
      "Epoch 51/100\n",
      "192/236 [=======================>......] - ETA: 0s - loss: 0.5736 - accuracy: 0.8661\n",
      "Epoch 00051: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5738 - accuracy: 0.8646 - val_loss: 0.5740 - val_accuracy: 0.8622\n",
      "Epoch 52/100\n",
      "206/236 [=========================>....] - ETA: 0s - loss: 0.5695 - accuracy: 0.8660\n",
      "Epoch 00052: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5699 - accuracy: 0.8650 - val_loss: 0.5703 - val_accuracy: 0.8628\n",
      "Epoch 53/100\n",
      "185/236 [======================>.......] - ETA: 0s - loss: 0.5659 - accuracy: 0.8647\n",
      "Epoch 00053: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5661 - accuracy: 0.8653 - val_loss: 0.5666 - val_accuracy: 0.8631\n",
      "Epoch 54/100\n",
      "226/236 [===========================>..] - ETA: 0s - loss: 0.5632 - accuracy: 0.8656\n",
      "Epoch 00054: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5624 - accuracy: 0.8658 - val_loss: 0.5631 - val_accuracy: 0.8641\n",
      "Epoch 55/100\n",
      "211/236 [=========================>....] - ETA: 0s - loss: 0.5572 - accuracy: 0.8660\n",
      "Epoch 00055: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5589 - accuracy: 0.8662 - val_loss: 0.5597 - val_accuracy: 0.8639\n",
      "Epoch 56/100\n",
      "177/236 [=====================>........] - ETA: 0s - loss: 0.5563 - accuracy: 0.8668\n",
      "Epoch 00056: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5554 - accuracy: 0.8668 - val_loss: 0.5564 - val_accuracy: 0.8643\n",
      "Epoch 57/100\n",
      "188/236 [======================>.......] - ETA: 0s - loss: 0.5545 - accuracy: 0.8669\n",
      "Epoch 00057: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5521 - accuracy: 0.8670 - val_loss: 0.5532 - val_accuracy: 0.8645\n",
      "Epoch 58/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.5487 - accuracy: 0.8674\n",
      "Epoch 00058: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5488 - accuracy: 0.8673 - val_loss: 0.5500 - val_accuracy: 0.8646\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/236 [==========================>...] - ETA: 0s - loss: 0.5448 - accuracy: 0.8690\n",
      "Epoch 00059: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5457 - accuracy: 0.8680 - val_loss: 0.5470 - val_accuracy: 0.8656\n",
      "Epoch 60/100\n",
      "230/236 [============================>.] - ETA: 0s - loss: 0.5421 - accuracy: 0.8690\n",
      "Epoch 00060: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5426 - accuracy: 0.8685 - val_loss: 0.5441 - val_accuracy: 0.8655\n",
      "Epoch 61/100\n",
      "221/236 [===========================>..] - ETA: 0s - loss: 0.5393 - accuracy: 0.8691\n",
      "Epoch 00061: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.8688 - val_loss: 0.5412 - val_accuracy: 0.8658\n",
      "Epoch 62/100\n",
      "189/236 [=======================>......] - ETA: 0s - loss: 0.5381 - accuracy: 0.8685\n",
      "Epoch 00062: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5367 - accuracy: 0.8694 - val_loss: 0.5384 - val_accuracy: 0.8662\n",
      "Epoch 63/100\n",
      "201/236 [========================>.....] - ETA: 0s - loss: 0.5350 - accuracy: 0.8696\n",
      "Epoch 00063: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5339 - accuracy: 0.8701 - val_loss: 0.5357 - val_accuracy: 0.8667\n",
      "Epoch 64/100\n",
      "184/236 [======================>.......] - ETA: 0s - loss: 0.5324 - accuracy: 0.8697\n",
      "Epoch 00064: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5312 - accuracy: 0.8703 - val_loss: 0.5331 - val_accuracy: 0.8667\n",
      "Epoch 65/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.5289 - accuracy: 0.8707\n",
      "Epoch 00065: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5285 - accuracy: 0.8707 - val_loss: 0.5305 - val_accuracy: 0.8672\n",
      "Epoch 66/100\n",
      "184/236 [======================>.......] - ETA: 0s - loss: 0.5249 - accuracy: 0.8714\n",
      "Epoch 00066: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5259 - accuracy: 0.8712 - val_loss: 0.5280 - val_accuracy: 0.8670\n",
      "Epoch 67/100\n",
      "192/236 [=======================>......] - ETA: 0s - loss: 0.5227 - accuracy: 0.8722\n",
      "Epoch 00067: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5233 - accuracy: 0.8718 - val_loss: 0.5255 - val_accuracy: 0.8677\n",
      "Epoch 68/100\n",
      "195/236 [=======================>......] - ETA: 0s - loss: 0.5223 - accuracy: 0.8714\n",
      "Epoch 00068: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5208 - accuracy: 0.8723 - val_loss: 0.5231 - val_accuracy: 0.8680\n",
      "Epoch 69/100\n",
      "198/236 [========================>.....] - ETA: 0s - loss: 0.5190 - accuracy: 0.8726\n",
      "Epoch 00069: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5184 - accuracy: 0.8726 - val_loss: 0.5208 - val_accuracy: 0.8679\n",
      "Epoch 70/100\n",
      "186/236 [======================>.......] - ETA: 0s - loss: 0.5179 - accuracy: 0.8722\n",
      "Epoch 00070: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5161 - accuracy: 0.8731 - val_loss: 0.5185 - val_accuracy: 0.8679\n",
      "Epoch 71/100\n",
      "186/236 [======================>.......] - ETA: 0s - loss: 0.5107 - accuracy: 0.8742\n",
      "Epoch 00071: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5138 - accuracy: 0.8733 - val_loss: 0.5163 - val_accuracy: 0.8679\n",
      "Epoch 72/100\n",
      "204/236 [========================>.....] - ETA: 0s - loss: 0.5127 - accuracy: 0.8731\n",
      "Epoch 00072: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5115 - accuracy: 0.8738 - val_loss: 0.5141 - val_accuracy: 0.8684\n",
      "Epoch 73/100\n",
      "212/236 [=========================>....] - ETA: 0s - loss: 0.5094 - accuracy: 0.8743\n",
      "Epoch 00073: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 945us/step - loss: 0.5093 - accuracy: 0.8740 - val_loss: 0.5120 - val_accuracy: 0.8687\n",
      "Epoch 74/100\n",
      "204/236 [========================>.....] - ETA: 0s - loss: 0.5092 - accuracy: 0.8741\n",
      "Epoch 00074: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5071 - accuracy: 0.8741 - val_loss: 0.5099 - val_accuracy: 0.8692\n",
      "Epoch 75/100\n",
      "204/236 [========================>.....] - ETA: 0s - loss: 0.5084 - accuracy: 0.8736\n",
      "Epoch 00075: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5050 - accuracy: 0.8746 - val_loss: 0.5079 - val_accuracy: 0.8694\n",
      "Epoch 76/100\n",
      "208/236 [=========================>....] - ETA: 0s - loss: 0.5043 - accuracy: 0.8749\n",
      "Epoch 00076: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5030 - accuracy: 0.8750 - val_loss: 0.5059 - val_accuracy: 0.8702\n",
      "Epoch 77/100\n",
      "211/236 [=========================>....] - ETA: 0s - loss: 0.5003 - accuracy: 0.8749\n",
      "Epoch 00077: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5009 - accuracy: 0.8752 - val_loss: 0.5040 - val_accuracy: 0.8701\n",
      "Epoch 78/100\n",
      "170/236 [====================>.........] - ETA: 0s - loss: 0.4947 - accuracy: 0.8768\n",
      "Epoch 00078: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4990 - accuracy: 0.8754 - val_loss: 0.5021 - val_accuracy: 0.8706\n",
      "Epoch 79/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.4995 - accuracy: 0.8753\n",
      "Epoch 00079: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4970 - accuracy: 0.8761 - val_loss: 0.5002 - val_accuracy: 0.8711\n",
      "Epoch 80/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.4953 - accuracy: 0.8765\n",
      "Epoch 00080: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4952 - accuracy: 0.8765 - val_loss: 0.4984 - val_accuracy: 0.8719\n",
      "Epoch 81/100\n",
      "198/236 [========================>.....] - ETA: 0s - loss: 0.4947 - accuracy: 0.8758\n",
      "Epoch 00081: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4933 - accuracy: 0.8767 - val_loss: 0.4966 - val_accuracy: 0.8724\n",
      "Epoch 82/100\n",
      "176/236 [=====================>........] - ETA: 0s - loss: 0.4884 - accuracy: 0.8791\n",
      "Epoch 00082: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4915 - accuracy: 0.8771 - val_loss: 0.4949 - val_accuracy: 0.8730\n",
      "Epoch 83/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.4896 - accuracy: 0.8774\n",
      "Epoch 00083: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4897 - accuracy: 0.8773 - val_loss: 0.4932 - val_accuracy: 0.8728\n",
      "Epoch 84/100\n",
      "202/236 [========================>.....] - ETA: 0s - loss: 0.4896 - accuracy: 0.8773\n",
      "Epoch 00084: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4879 - accuracy: 0.8777 - val_loss: 0.4915 - val_accuracy: 0.8724\n",
      "Epoch 85/100\n",
      "206/236 [=========================>....] - ETA: 0s - loss: 0.4815 - accuracy: 0.8799\n",
      "Epoch 00085: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 948us/step - loss: 0.4862 - accuracy: 0.8776 - val_loss: 0.4899 - val_accuracy: 0.8726\n",
      "Epoch 86/100\n",
      "187/236 [======================>.......] - ETA: 0s - loss: 0.4864 - accuracy: 0.8772\n",
      "Epoch 00086: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4846 - accuracy: 0.8781 - val_loss: 0.4883 - val_accuracy: 0.8731\n",
      "Epoch 87/100\n",
      "193/236 [=======================>......] - ETA: 0s - loss: 0.4854 - accuracy: 0.8768\n",
      "Epoch 00087: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4829 - accuracy: 0.8783 - val_loss: 0.4867 - val_accuracy: 0.8731\n",
      "Epoch 88/100\n",
      "194/236 [=======================>......] - ETA: 0s - loss: 0.4806 - accuracy: 0.8789\n",
      "Epoch 00088: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4813 - accuracy: 0.8785 - val_loss: 0.4851 - val_accuracy: 0.8733\n",
      "Epoch 89/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.4799 - accuracy: 0.8784\n",
      "Epoch 00089: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4797 - accuracy: 0.8787 - val_loss: 0.4836 - val_accuracy: 0.8736\n",
      "Epoch 90/100\n",
      "185/236 [======================>.......] - ETA: 0s - loss: 0.4771 - accuracy: 0.8787\n",
      "Epoch 00090: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4781 - accuracy: 0.8787 - val_loss: 0.4821 - val_accuracy: 0.8740\n",
      "Epoch 91/100\n",
      "225/236 [===========================>..] - ETA: 0s - loss: 0.4777 - accuracy: 0.8784\n",
      "Epoch 00091: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4766 - accuracy: 0.8789 - val_loss: 0.4807 - val_accuracy: 0.8741\n",
      "Epoch 92/100\n",
      "200/236 [========================>.....] - ETA: 0s - loss: 0.4762 - accuracy: 0.8789\n",
      "Epoch 00092: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4751 - accuracy: 0.8792 - val_loss: 0.4792 - val_accuracy: 0.8743\n",
      "Epoch 93/100\n",
      "218/236 [==========================>...] - ETA: 0s - loss: 0.4741 - accuracy: 0.8800\n",
      "Epoch 00093: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4736 - accuracy: 0.8795 - val_loss: 0.4778 - val_accuracy: 0.8747\n",
      "Epoch 94/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.4708 - accuracy: 0.8798\n",
      "Epoch 00094: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4722 - accuracy: 0.8797 - val_loss: 0.4764 - val_accuracy: 0.8750\n",
      "Epoch 95/100\n",
      "205/236 [=========================>....] - ETA: 0s - loss: 0.4687 - accuracy: 0.8806\n",
      "Epoch 00095: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4708 - accuracy: 0.8800 - val_loss: 0.4750 - val_accuracy: 0.8752\n",
      "Epoch 96/100\n",
      "225/236 [===========================>..] - ETA: 0s - loss: 0.4692 - accuracy: 0.8803\n",
      "Epoch 00096: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.8799 - val_loss: 0.4737 - val_accuracy: 0.8753\n",
      "Epoch 97/100\n",
      "195/236 [=======================>......] - ETA: 0s - loss: 0.4695 - accuracy: 0.8805\n",
      "Epoch 00097: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4680 - accuracy: 0.8804 - val_loss: 0.4724 - val_accuracy: 0.8755\n",
      "Epoch 98/100\n",
      "217/236 [==========================>...] - ETA: 0s - loss: 0.4651 - accuracy: 0.8812\n",
      "Epoch 00098: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4666 - accuracy: 0.8806 - val_loss: 0.4711 - val_accuracy: 0.8755\n",
      "Epoch 99/100\n",
      "185/236 [======================>.......] - ETA: 0s - loss: 0.4671 - accuracy: 0.8802\n",
      "Epoch 00099: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4653 - accuracy: 0.8808 - val_loss: 0.4698 - val_accuracy: 0.8755\n",
      "Epoch 100/100\n",
      "185/236 [======================>.......] - ETA: 0s - loss: 0.4621 - accuracy: 0.8809\n",
      "Epoch 00100: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4640 - accuracy: 0.8810 - val_loss: 0.4686 - val_accuracy: 0.8760\n",
      "  1/394 [..............................] - ETA: 0s - loss: 0.5907 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "394/394 [==============================] - 0s 550us/step - loss: 0.4789 - accuracy: 0.8760\n",
      "[0.47885289788246155, 0.8759523630142212]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential  # keras model\n",
    "from tensorflow.keras.layers import Flatten, Dense  # Flatten(Input Layer)\n",
    "                                                    # Dense(Output Layer)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "\n",
    "# Data Split\n",
    "# 기존에는 test_x_data, test_t_data 이 두 데이터를 validation 용도로 \n",
    "# 사용했어요!\n",
    "# 이제는 test_x_data, test_t_data 이 두 데이터를 test 용도로 사용할꺼예요!\n",
    "# 최종 모델 성능평가를 위해서 딱 1번만 사용할꺼예요!\n",
    "# 그러면 validation은 어떻게 하나요?\n",
    "# keras는 학습할 때 train data를 일정부분 나누어서 자체 validation이 가능\n",
    "# keras 기능을 이용해서 validation 처리\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)\n",
    "\n",
    "# 우리는 loss 지정할 때 sparse_categorical_crossentropy로 loss함수를\n",
    "# 지정할 예정이기 때문에 label에 대한 one-hot encoding처리가 필요 없어요!\n",
    "\n",
    "# model 생성\n",
    "model = Sequential()\n",
    "\n",
    "# layer 추가\n",
    "# input layer\n",
    "model.add(Flatten(input_shape=(norm_train_x_data.shape[1],)))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=10,\n",
    "                activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model의 저장할려고 해요. model 구조 빼고 checkpoint기능을 이용해서 \n",
    "# weight, b만 저장\n",
    "# 어디에 저장할지를 알려줘야 해요!\n",
    "checkpoint_path = './training_ckpt/cp.ckpt'\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)  # 실제 경로로 만들어요!\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                              save_weights_only=True,\n",
    "                              verbose=1)\n",
    "\n",
    "\n",
    "# 학습결과를 변수에 저장\n",
    "history = model.fit(norm_train_x_data,\n",
    "                    train_t_data,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[cp_callback])\n",
    "\n",
    "# 우리 모델에 대한 최종 평가진행\n",
    "print(model.evaluate(norm_test_x_data, test_t_data))\n",
    "#        loss               accuracy\n",
    "# [0.4799305200576782, 0.876031756401062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d336ea9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 0s 658us/step - loss: 2.4018 - accuracy: 0.1164\n",
      "[2.401752233505249, 0.11642856895923615]\n"
     ]
    }
   ],
   "source": [
    "# 아하 이렇게 저장 가능\n",
    "# 불러서 다시 사용하려면 어떻게 해야하나요?\n",
    "\n",
    "# 일단 학습하지 않은 상태로 evaluation을 진행하면 당연히 평가결과가\n",
    "# 좋지않음. 이거 확인하고\n",
    "# 그 다음에 checkpoint 파일을 로드해서 model을 재설정하고 평가를 진행\n",
    "# 좋게 나오겠네요!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential  # keras model\n",
    "from tensorflow.keras.layers import Flatten, Dense  # Flatten(Input Layer)\n",
    "                                                    # Dense(Output Layer)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "\n",
    "# Data Split\n",
    "# 기존에는 test_x_data, test_t_data 이 두 데이터를 validation 용도로 \n",
    "# 사용했어요!\n",
    "# 이제는 test_x_data, test_t_data 이 두 데이터를 test 용도로 사용할꺼예요!\n",
    "# 최종 모델 성능평가를 위해서 딱 1번만 사용할꺼예요!\n",
    "# 그러면 validation은 어떻게 하나요?\n",
    "# keras는 학습할 때 train data를 일정부분 나누어서 자체 validation이 가능\n",
    "# keras 기능을 이용해서 validation 처리\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)\n",
    "\n",
    "# 우리는 loss 지정할 때 sparse_categorical_crossentropy로 loss함수를\n",
    "# 지정할 예정이기 때문에 label에 대한 one-hot encoding처리가 필요 없어요!\n",
    "\n",
    "# model 생성\n",
    "model = Sequential()\n",
    "\n",
    "# layer 추가\n",
    "# input layer\n",
    "model.add(Flatten(input_shape=(norm_train_x_data.shape[1],)))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=10,\n",
    "                activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 원래는 학습을 진행행해야 해요! 그런데 학습을 안할꺼예요!\n",
    "\n",
    "# 학습을 진행하지 않고 최종 평가진행\n",
    "print(model.evaluate(norm_test_x_data, test_t_data))\n",
    "#        loss               accuracy\n",
    "# [2.4244349002838135, 0.08492063730955124]\n",
    "# 당연히 학습이 안된 모델이기 때문에 이렇게 나오는게 정상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0b1e70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 0s 572us/step - loss: 0.4789 - accuracy: 0.8760\n",
      "[0.47885289788246155, 0.8759523630142212]\n"
     ]
    }
   ],
   "source": [
    "# 이번에는 checkpoint 파일에 있는 weight를 load한 후\n",
    "# evaluation 시켜보아요!\n",
    "\n",
    "checkpoint_path = './training_ckpt/cp.ckpt'\n",
    "model.load_weights(checkpoint_path)\n",
    "print(model.evaluate(norm_test_x_data, test_t_data))\n",
    "# [0.48125016689300537, 0.8734920620918274]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-machine_TF2]",
   "language": "python",
   "name": "conda-env-.conda-machine_TF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
