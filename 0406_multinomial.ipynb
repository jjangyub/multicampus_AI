{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8abc6e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logisitic Regression Model의 정확도 : 0.9473684210526315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeop\\.conda\\envs\\machine_TF15\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 36\n"
     ]
    }
   ],
   "source": [
    "# 위스콘신 데이터 이용\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# raw data\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "x_data = cancer.data  # 2차원 ndarray - 독립변수, feature\n",
    "t_data = cancer.target  # 1차원 ndarray - 종속변수, label\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(x_data,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=2)\n",
    "\n",
    "# model 생성\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "# model 학습\n",
    "model.fit(train_x_data, train_t_data)\n",
    "\n",
    "# accuracy로 model 평가\n",
    "test_score = model.score(test_x_data, test_t_data)\n",
    "\n",
    "print('Logisitic Regression Model의 정확도 : {}'.format(test_score))\n",
    "# 0.9473684210526315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3661bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDCleassifier의 정확도 : 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Raw Data Set Loading\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "x_data = cancer.data  # 2차원 ndarray - 독립변수, feature\n",
    "t_data = cancer.target  # 1차원 ndarray - 종속변수, label\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(x_data,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=2)\n",
    "\n",
    "# model 생성\n",
    "sgd = linear_model.SGDClassifier(loss='log',   # logistic regression을 이용\n",
    "                                 tol=1e-5,     # tol값만큼만 차이나면 그만 진행\n",
    "                                 random_state=2)     \n",
    "\n",
    "# model 학습\n",
    "sgd.fit(train_x_data, train_t_data)\n",
    "\n",
    "# accuracy 측정\n",
    "test_score = sgd.score(test_x_data, test_t_data)\n",
    "\n",
    "print('SGDCleassifier의 정확도 : {}'.format(test_score))  #  0.8947368421052632\n",
    "# 왜 그러지? => 정규화 안함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1338a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화를 이용한 SGDClassifier의 정확도 : 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Raw Data Set Loading\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "x_data = cancer.data     # 2차원 ndarray - 독립변수, feature\n",
    "t_data = cancer.target   # 1차원 ndarray - 종속변수, label\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(x_data,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=2)\n",
    "\n",
    "# Data 정규화\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "# Model 생성\n",
    "sgd = linear_model.SGDClassifier(loss='log',   # logistic regression을 이용\n",
    "                                 tol=1e-5,     # 얼마나 반복할건지를 loss값으로 설정 \n",
    "                                 random_state=2)\n",
    "# Model 학습\n",
    "sgd.fit(scaler.transform(train_x_data), train_t_data)\n",
    "\n",
    "# Accuracy 측정\n",
    "test_score = sgd.score(scaler.transform(test_x_data), test_t_data)\n",
    "\n",
    "print('정규화를 이용한 SGDClassifier의 정확도 : {}'.format(test_score))\n",
    "# 0.9649122807017544\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a546a8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDCleassifier의 정확도 : 0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "# 위 코드에 L2 규제 포함\n",
    "\n",
    "x_data = cancer.data  # 2차원 ndarray - 독립변수, feature\n",
    "t_data = cancer.target  # 1차원 ndarray - 종속변수, label\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(x_data,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=2)\n",
    "\n",
    "# model 생성\n",
    "sgd = linear_model.SGDClassifier(loss='log',   # logistic regression을 이용\n",
    "                                 tol=1e-5,     # tol값만큼만 차이나면 그만 진행\n",
    "                                 penalty='l2', # L2 규제 이용\n",
    "                                 alpha=0.001,  # 규제 강도\n",
    "                                 random_state=2\n",
    "                                 )     \n",
    "\n",
    "# data 정규화\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "# model 학습\n",
    "sgd.fit(scaler.transform(train_x_data), train_t_data)\n",
    "\n",
    "# accuracy 측정\n",
    "test_score = sgd.score(scaler.transform(test_x_data), test_t_data)\n",
    "\n",
    "print('SGDCleassifier의 정확도 : {}'.format(test_score))\n",
    "# 0.9707602339181286\n",
    "# 규제를 이용하면 조금 더 나은 모델 만들기 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46359736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>161</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  height  weight\n",
       "0      1     188      71\n",
       "1      2     161      68\n",
       "2      0     178      52\n",
       "3      2     136      63\n",
       "4      1     145      52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 3)\n"
     ]
    }
   ],
   "source": [
    "# multinomial (다항분류)\n",
    "%reset\n",
    "\n",
    "# BMI 예제 - sklearn으로 먼저 구현하고 성능평가 진행\n",
    "# 성능평가의 metric은 accuracy로 진행\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "# raw data\n",
    "df = pd.read_csv('./data/bmi.csv',skiprows=3)\n",
    "display(df.head())\n",
    "print(df.shape) # (20000, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85e44c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label     0\n",
      "height    0\n",
      "weight    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "# 결측치\n",
    "print(df.isnull().sum())  # 결측치 없음\n",
    "\n",
    "# 이상치\n",
    "# z-score방식 사용\n",
    "zscore_threshold = 2.0\n",
    "\n",
    "# (np.abs(stats.zscore(df['height'])) > zscore_threshold).sum() # 0 이상치 없음\n",
    "# (np.abs(stats.zscore(df['weight'])) > zscore_threshold).sum()  # 0 이상치 없음\n",
    "# np.unique(df['label'], return_counts=True)\n",
    "# 이상치 없고 데이터의 편향도 존재X, 상태가 좋은 데이터임\n",
    "\n",
    "# 정규화\n",
    "# train data, validation data분리 후 정규화 진행\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df[['height','weight']],\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48d94fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn으로 구한 Accuracy : 0.9845\n",
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeop\\.conda\\envs\\machine_TF15\\lib\\site-packages\\sklearn\\base.py:451: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "# Model 생성 후 학습 및 평가\n",
    "model = linear_model.LogisticRegression(C=100000)\n",
    "# 규제를 적용할 수 있어요(L2 규제)\n",
    "# alpha값은 우리가 정해줘야 해요! \n",
    "# C = 1 / alpha\n",
    "\n",
    "model.fit(norm_train_x_data, train_t_data)\n",
    "\n",
    "# 평가를 위한 예측결과를 얻어내요!\n",
    "predict_val = model.predict(norm_test_x_data)\n",
    "# 이렇게 나온 예측결과와 test_t_data를 비교해야 해요!!\n",
    "acc = accuracy_score(predict_val, test_t_data)\n",
    "\n",
    "print('sklearn으로 구한 Accuracy : {}'.format(acc))\n",
    "# 0.9845\n",
    "\n",
    "# prediction\n",
    "result = model.predict(scaler.transform(np.array([[187, 81]])))\n",
    "print(result)  # [1] 표준이네요!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d9f70c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value : 0.9484084844589233\n",
      "loss value : 0.16480077803134918\n",
      "loss value : 0.12274885177612305\n",
      "loss value : 0.10345945507287979\n",
      "loss value : 0.09191364049911499\n",
      "loss value : 0.08406415581703186\n",
      "loss value : 0.07830533385276794\n",
      "loss value : 0.07385975122451782\n",
      "loss value : 0.07029939442873001\n",
      "loss value : 0.06736902892589569\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow를 이용해서 구현해 보아요!\n",
    "\n",
    "# multinomial 문제이기 때문에 label을 one-hot encoding처리 해야 해요!\n",
    "# train_t_data, test_t_data를 one-hot encoding으로 변경할건데..\n",
    "# tensorflow의 기능을 이용해서 변경 => tensorflow node로 생성.\n",
    "sess = tf.Session()\n",
    "\n",
    "onehot_train_t_data = sess.run(tf.one_hot(train_t_data, depth=3))  # depth는 class의 개수\n",
    "onehot_test_t_data = sess.run(tf.one_hot(test_t_data, depth=3))  # depth는 class의 개수\n",
    "\n",
    "# tensorflow graph를 그려보아요!\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([2,3]))\n",
    "b = tf.Variable(tf.random.normal([3]))\n",
    "\n",
    "# model, Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,\n",
    "                                                                 labels=T))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# session, 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 반복 학습\n",
    "# 그런데 반복학습할때 주의해야 할 점이 있어요!!\n",
    "# 학습데이터의 사이즈가 매우 크면 메모리에 데이터를 한번에 모두 loading할 수 \n",
    "# 없어요. memory fault나면서 수행이 중지!\n",
    "# 어떻게 해결해야 하나요? => batch처리를 해야 해요!\n",
    "num_of_epoch = 1000 # 학습을 위한 전체 epoch 수\n",
    "num_of_batch = 100   # 한번에 학습할 데이터 량\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    total_batch = int(norm_train_x_data.shape[0] / num_of_batch)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_x = norm_train_x_data[i*num_of_batch:(i+1)*num_of_batch]\n",
    "        batch_y = onehot_train_t_data[i*num_of_batch:(i+1)*num_of_batch]\n",
    "        _, loss_val = sess.run([train, loss], \n",
    "                               feed_dict={X:batch_x,\n",
    "                                          T:batch_y})                           \n",
    "    if step % 100 == 0:\n",
    "        print('loss value : {}'.format(loss_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d3527f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.757560e-05 9.079327e-01 9.201969e-02]]\n",
      "[1]\n",
      "0.9855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeop\\.conda\\envs\\machine_TF15\\lib\\site-packages\\sklearn\\base.py:451: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "# 학습 종료됨\n",
    "\n",
    "# 성능평가(Accuracy)를 해야 해요!\n",
    "result = sess.run(H, feed_dict={X:scaler.transform(np.array([[187,81]]))})\n",
    "print(result) # 세개 값 중에 값이 가장 큰 것아 확률이 높은것을 의미\n",
    "print(np.argmax(result, axis=1))  # 가장 큰 값의 index를 알려줘요!\n",
    "\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "result = sess.run(accuracy, feed_dict={X:norm_test_x_data,\n",
    "                                       T:onehot_test_t_data})\n",
    "print(result)  # 0.9855"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-machine_TF15]",
   "language": "python",
   "name": "conda-env-.conda-machine_TF15-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
