{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28586b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model의 정확도 : 0.9473684210526315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\campusseven02\\anaconda3\\envs\\machine_TF15\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "# 위스콘신 유방암 데이터셋을 이용해서 Logistic Regression을 구현해 보아요!\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Raw Data Set Loading\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "x_data = cancer.data     # 2차원 ndarray - 독립변수, feature\n",
    "t_data = cancer.target   # 1차원 ndarray - 종속변수, label\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(x_data,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=2)\n",
    "\n",
    "# Model 생성\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "# Model 학습\n",
    "model.fit(train_x_data, train_t_data)\n",
    "\n",
    "# accuracy로 model 평가\n",
    "test_score = model.score(test_x_data, test_t_data)\n",
    "\n",
    "print('Logistic Regression Model의 정확도 : {}'.format(test_score))\n",
    "# 0.9473684210526315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b926c373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier의 정확도 : 0.8947368421052632\n",
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Raw Data Set Loading\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "x_data = cancer.data     # 2차원 ndarray - 독립변수, feature\n",
    "t_data = cancer.target   # 1차원 ndarray - 종속변수, label\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(x_data,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=2)\n",
    "\n",
    "# Model 생성\n",
    "sgd = linear_model.SGDClassifier(loss='log',   # logistic regression을 이용\n",
    "                                 tol=1e-5,     # 얼마나 반복할건지를 loss값으로 설정 \n",
    "                                 random_state=2)\n",
    "# Model 학습\n",
    "sgd.fit(train_x_data, train_t_data)\n",
    "\n",
    "# Accuracy 측정\n",
    "test_score = sgd.score(test_x_data, test_t_data)\n",
    "\n",
    "print('SGDClassifier의 정확도 : {}'.format(test_score))\n",
    "# 0.8947368421052632\n",
    "# 왜 그럴까...???\n",
    "# 정규화 안했어요!  => 각 feature마다 scale이 제각각이예요!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2893cc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화를 이용한 SGDClassifier의 정확도 : 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Raw Data Set Loading\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "x_data = cancer.data     # 2차원 ndarray - 독립변수, feature\n",
    "t_data = cancer.target   # 1차원 ndarray - 종속변수, label\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(x_data,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=2)\n",
    "\n",
    "# Data 정규화\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "# Model 생성\n",
    "sgd = linear_model.SGDClassifier(loss='log',   # logistic regression을 이용\n",
    "                                 tol=1e-5,     # 얼마나 반복할건지를 loss값으로 설정 \n",
    "                                 random_state=2)\n",
    "# Model 학습\n",
    "sgd.fit(scaler.transform(train_x_data), train_t_data)\n",
    "\n",
    "# Accuracy 측정\n",
    "test_score = sgd.score(scaler.transform(test_x_data), test_t_data)\n",
    "\n",
    "print('정규화를 이용한 SGDClassifier의 정확도 : {}'.format(test_score))\n",
    "# 0.9649122807017544\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eae143e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화를 이용한 SGDClassifier의 정확도 : 0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "# 위의 코드에 L2 Regularization(규제)도 포함해 보아요!\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Raw Data Set Loading\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "x_data = cancer.data     # 2차원 ndarray - 독립변수, feature\n",
    "t_data = cancer.target   # 1차원 ndarray - 종속변수, label\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(x_data,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=2)\n",
    "\n",
    "# Data 정규화\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "# Model 생성\n",
    "sgd = linear_model.SGDClassifier(loss='log',   # logistic regression을 이용\n",
    "                                 tol=1e-5,     # 얼마나 반복할건지를 loss값으로 설정 \n",
    "                                 penalty='l2', # L2 규제를 이용할꺼예요! \n",
    "                                 alpha=0.001,  # 규제 강도     \n",
    "                                 random_state=2)\n",
    "# Model 학습\n",
    "sgd.fit(scaler.transform(train_x_data), train_t_data)\n",
    "\n",
    "# Accuracy 측정\n",
    "test_score = sgd.score(scaler.transform(test_x_data), test_t_data)\n",
    "\n",
    "print('정규화를 이용한 SGDClassifier의 정확도 : {}'.format(test_score))\n",
    "# 0.9649122807017544\n",
    "# 규제를 이용하면 조금 더 나은 모델을 만들 수 있어요!\n",
    "# 0.9707602339181286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "07ecbf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# BMI 예제 구현 - sklearn으로 먼저 구현하고 성능평가를 진행\n",
    "# 성능평가의 metric은 accuracy로 진행\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/bmi.csv', skiprows=3)\n",
    "# display(df.head())\n",
    "# print(df.shape)   # (20000, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "68d44d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 전처리\n",
    "\n",
    "## 결측치 체크부터 해야해요!\n",
    "# print(df.isnull().sum())  # 데이터에 결측치가 없어요!!\n",
    "\n",
    "## 이상치 체크하고 이상치가 존재하면 처리해야 해요!\n",
    "## z-score방식으로 알아보아요!\n",
    "# zscore_threshold = 2.0\n",
    "\n",
    "# (np.abs(stats.zscore(df['height'])) > zscore_threshold).sum() # => 0이면 이상치가 없어요!\n",
    "# (np.abs(stats.zscore(df['weight'])) > zscore_threshold).sum() # => 0이면 이상치가 없어요!\n",
    "# np.unique(df['label'], return_counts=True)\n",
    "# 이상치도 없고 데이터의 편향도 존재하지 않아요! 상태가 좋은 데이터예요!\n",
    "\n",
    "# 정규화(Normalization)를 해야 해요!\n",
    "# 일단 먼저 train data와 validation data를 분리한 후 정규화를 진행!\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df[['height', 'weight']],\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b71523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn으로 구한 Accuracy : 0.9845\n",
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\campusseven02\\anaconda3\\envs\\machine_TF15\\lib\\site-packages\\sklearn\\base.py:451: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "# Model 생성 후 학습 및 평가\n",
    "model = linear_model.LogisticRegression(C=100000)\n",
    "# 규제를 적용할 수 있어요(L2 규제)\n",
    "# alpha값은 우리가 정해줘야 해요! \n",
    "# C = 1 / alpha\n",
    "\n",
    "model.fit(norm_train_x_data, train_t_data)\n",
    "\n",
    "# 평가를 위한 예측결과를 얻어내요!\n",
    "predict_val = model.predict(norm_test_x_data)\n",
    "# 이렇게 나온 예측결과와 test_t_data를 비교해야 해요!!\n",
    "acc = accuracy_score(predict_val, test_t_data)\n",
    "\n",
    "print('sklearn으로 구한 Accuracy : {}'.format(acc))\n",
    "# 0.9851666666666666\n",
    "\n",
    "# prediction\n",
    "result = model.predict(scaler.transform(np.array([[187, 81]])))\n",
    "print(result)  # [1] 표준이네요!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "da73b86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 64\n",
      "loss value : 0.8950161933898926\n",
      "loss value : 0.1648642122745514\n",
      "loss value : 0.12277288734912872\n",
      "loss value : 0.10347263514995575\n",
      "loss value : 0.09192224591970444\n",
      "loss value : 0.08407024294137955\n",
      "loss value : 0.07830987870693207\n",
      "loss value : 0.07386329770088196\n",
      "loss value : 0.07030230760574341\n",
      "loss value : 0.06737145781517029\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow를 이용해서 구현해 보아요!\n",
    "\n",
    "# multinomial 문제이기 때문에 label을 one-hot encoding처리 해야 해요!\n",
    "# train_t_data, test_t_data를 one-hot encoding으로 변경할건데..\n",
    "# tensorflow의 기능을 이용해서 변경 => tensorflow node로 생성.\n",
    "sess = tf.Session()\n",
    "\n",
    "onehot_train_t_data = sess.run(tf.one_hot(train_t_data, depth=3))  # depth는 class의 개수\n",
    "onehot_test_t_data = sess.run(tf.one_hot(test_t_data, depth=3))  # depth는 class의 개수\n",
    "\n",
    "# tensorflow graph를 그려보아요!\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([2,3]))\n",
    "b = tf.Variable(tf.random.normal([3]))\n",
    "\n",
    "# model, Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,\n",
    "                                                                 labels=T))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# session, 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 반복 학습\n",
    "# 그런데 반복학습할때 주의해야 할 점이 있어요!!\n",
    "# 학습데이터의 사이즈가 매우 크면 메모리에 데이터를 한번에 모두 loading할 수 \n",
    "# 없어요. memory fault나면서 수행이 중지!\n",
    "# 어떻게 해결해야 하나요? => batch처리를 해야 해요!\n",
    "num_of_epoch = 1000 # 학습을 위한 전체 epoch 수\n",
    "num_of_batch = 100   # 한번에 학습할 데이터 량\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    total_batch = int(norm_train_x_data.shape[0] / num_of_batch)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_x = norm_train_x_data[i*num_of_batch:(i+1)*num_of_batch]\n",
    "        batch_y = onehot_train_t_data[i*num_of_batch:(i+1)*num_of_batch]\n",
    "        _, loss_val = sess.run([train, loss], \n",
    "                               feed_dict={X:batch_x,\n",
    "                                          T:batch_y})                           \n",
    "    if step % 100 == 0:\n",
    "        print('loss value : {}'.format(loss_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c7297120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9855\n"
     ]
    }
   ],
   "source": [
    "# 학습이 종료되었어요!\n",
    "\n",
    "# 성능평가(Accuracy)를 해야 해요!\n",
    "# result = sess.run(H, feed_dict={X:scaler.transform(np.array([[187,81]]))})\n",
    "# print(result)\n",
    "# print(np.argmax(result, axis=1))  # 가장 큰 값의 index를 알려줘요!\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "result = sess.run(accuracy, feed_dict={X:norm_test_x_data,\n",
    "                                       T:onehot_test_t_data})\n",
    "print(result)  # 0.9855"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-machine_TF15] *",
   "language": "python",
   "name": "conda-env-.conda-machine_TF15-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
